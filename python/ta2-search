#!/usr/bin/env python3

"""
Command Line Interface for running the DSBox TA2 Search
"""

from dsbox_dev_setup import path_setup
path_setup()

import argparse
import copy
import sys
import os
import pandas
import json
import shutil
import signal
import sklearn.externals

from dsbox.executer.executionhelper import ExecutionHelper
from dsbox.planner.common.data_manager import Dataset, DataManager
from dsbox.planner.common.problem_manager import Problem
from dsbox.planner.controller import Controller, Feature
from dsbox.schema.data_profile import DataProfile
from dsbox.planner.event_handler import PlannerEventHandler

from pathlib import Path

TIMEOUT = 25*60 # Timeout after 25 minutes

DEBUG = 0
LIB_DIRECTORY = os.path.dirname(os.path.realpath(__file__)) + "/library"

def main(argv=None): # IGNORE:C0111
    '''Command line options.'''

    if argv is None:
        argv = sys.argv
    else:
        sys.argv.extend(argv)

    program_name = os.path.basename(sys.argv[0])
    program_shortdesc = __import__('__main__').__doc__.split("\n")[1]
    program_usage = '''%s
USAGE
ta2-search <search_config_file>
''' % program_shortdesc

    if len(sys.argv) < 2:
        print(program_usage)
        exit(1)

    conf_file = sys.argv[1]
    config = {}
    with open(conf_file) as conf_data:
        config = json.load(conf_data)
        conf_data.close()

    if "timeout" in config:
        # Timeout less 60 seconds, to give system chance to clean up
        TIMEOUT = int(config.get("timeout"))*60 - 60

    # Start the controller
    controller = Controller(LIB_DIRECTORY)
    controller.initialize_from_config(config)
    controller.load_problem()

    # Setup a signal handler to exit gracefully
    # Either on an interrupt or after a certain time
    def write_results_and_exit(signal, frame):
        controller.write_training_results()
        sys.exit(0)
    signal.signal(signal.SIGINT, write_results_and_exit)
    signal.signal(signal.SIGTERM, write_results_and_exit)
    signal.signal(signal.SIGALRM, write_results_and_exit)
    signal.alarm(TIMEOUT)

    # Load in data
    controller.initialize_training_data_from_config()

    # Get data details
    df = copy.copy(controller.data_manager.input_data)
    df_lbl = copy.copy(controller.data_manager.target_data)
    df_profile = DataProfile(df)
    print(df_profile)

    for i in range(10):
        print("*")

    # Start training
    controller.initialize_planners()
    for result in controller.train(PlannerEventHandler()):
        if result == False:
            print("ProblemNotImplemented")
            sys.exit(148)
        pass

    return

    # Start testing
    problem = Problem()
    problem.load_problem(config["problem_root"], config["problem_schema"])

    # FIXME: considering test_data_root as training_data_root
    if "test_data_root" not in config:
        config["test_data_root"] = config["training_data_root"]

    # FIXME HACKY - to save space, output results in home directory and remove current outputs folder
#    home = "/nas/home/stan"
#    problem_name = problem.prID.rsplit("_", 1)[0]
#    output_file = home + "/outputs/" + problem_name + ".txt"
    output_file = config["temp_storage_root"] + "train+test.txt"
    f = open(output_file, "w+")

    # Load in the dataset, data manager and execution helper
    dataset = Dataset()
    dataset.load_dataset(config["test_data_root"], config["dataset_schema"])

    data_manager = DataManager()
    data_manager.initialize_data(problem, [dataset], view='TEST')

    hp = ExecutionHelper(problem, data_manager)

    initial_testdata = data_manager.input_data

    # for each pipeline compute its result on the test data by looking at the pickle files
    for ppln in controller.exec_pipelines:
        print("Processing", ppln.primitives) 

        primfile_base = config["temp_storage_root"] + "/models/" + ppln.id
        num_primitives = len(ppln.primitives)

        curr_testdata = initial_testdata.copy()
        last_primitive = None

        for i in range(num_primitives):
            primfile = primfile_base + ".primitive_" + str(i + 1) + ".pkl"

            primitive = sklearn.externals.joblib.load(primfile)
            last_primitive = primitive

            if primitive.task == "PreProcessing":
                new_testdata = hp.test_execute_primitive(primitive, curr_testdata)
            elif primitive.task == "FeatureExtraction":
                new_testdata = hp.test_featurise(primitive, curr_testdata)
            elif primitive.task == "Modeling":
                continue

            curr_testdata = new_testdata.copy()

        assert last_primitive != None

        result = pandas.DataFrame(last_primitive.executables.produce(inputs=curr_testdata).value, index = curr_testdata.index, \
               columns = [cn["colName"] for cn in data_manager.target_columns]) 

        # For each metric, compute the result
        for metric in problem.metrics:
            metric_function = problem._get_metric_function(metric)

            training_score = ppln.planner_result.metric_values[metric.name]
            test_score = hp._call_function(metric_function, data_manager.target_data, result)

            f.write("%s %s %s %s %s\n" % (metric.name, ppln.id, ppln.primitives, training_score, test_score))

    f.close()

#    print("Cleaning temp folder...")

#    temp_output_folder = config["temp_storage_root"].rsplit("/", 1)[0]
#    shutil.move(temp_output_folder, home + "/outputs")

#    print("Succesfully removed", temp_output_folder)
        
                                    
if __name__ == "__main__":
    if DEBUG:
        sys.argv.append("-h")
        sys.argv.append("-v")
    sys.exit(main())
