#!/usr/bin/env python3

"""
Command Line Interface for running the DSBox TA2 Search
"""

from dsbox_dev_setup import path_setup
path_setup()

import sys
import os
import pandas
import json
import signal
import sklearn.externals

from dsbox.executer.executionhelper import ExecutionHelper
from dsbox.planner.common.data_manager import Dataset, DataManager
from dsbox.planner.common.problem_manager import Problem
from dsbox.planner.controller import Controller, Feature
from dsbox.planner.event_handler import PlannerEventHandler

TIMEOUT = 25*60 # Timeout after 25 minutes

DEBUG = 0
LIB_DIRECTORY = os.path.dirname(os.path.realpath(__file__)) + "/library"

def main(argv=None): # IGNORE:C0111
    '''Command line options.'''

    if argv is None:
        argv = sys.argv
    else:
        sys.argv.extend(argv)

    program_name = os.path.basename(sys.argv[0])
    program_shortdesc = __import__('__main__').__doc__.split("\n")[1]
    program_usage = '''%s
USAGE
ta2-search <search_config_file>
''' % program_shortdesc

    if len(sys.argv) < 2:
        print(program_usage)
        exit(1)

    conf_file = sys.argv[1]
    config = {}
    with open(conf_file) as conf_data:
        config = json.load(conf_data)
        conf_data.close()

    if "timeout" in config:
        # Timeout less 60 seconds, to give system chance to clean up
        TIMEOUT = int(config.get("timeout"))*60 - 60

    # Start the controller
    controller = Controller(LIB_DIRECTORY)
    controller.initialize_from_config(config)
    controller.load_problem()

    # Setup a signal handler to exit gracefully
    # Either on an interrupt or after a certain time
    def write_results_and_exit(signal, frame):
        controller.write_training_results()
        sys.exit(0)
    signal.signal(signal.SIGINT, write_results_and_exit)
    signal.signal(signal.SIGTERM, write_results_and_exit)
    signal.signal(signal.SIGALRM, write_results_and_exit)
    signal.alarm(TIMEOUT)

    # Load in data
    controller.initialize_training_data_from_config()

    # Start training
    controller.initialize_planners()
    for result in controller.train(PlannerEventHandler()):
        if result == False:
            print("ProblemNotImplemented")
            sys.exit(148)
        pass

    # Do testing
    print()
    for ppln in controller.exec_pipelines:
        print(ppln, ppln.id, len(ppln.primitives))
    print()

    print(config)
    print(config["problem_schema"])
    print(config["dataset_schema"])

    print(controller.pipelinesfile)

    print()

    problem = Problem()
    problem.load_problem(config["problem_root"], config["problem_schema"])

    print(problem)

    # FIXME: considering test_data_root as training_data_root
    if "test_data_root" not in config:
        config["test_data_root"] = config["training_data_root"]

    dataset = Dataset()
    dataset.load_dataset(config["test_data_root"], config["dataset_schema"])

    print(dataset)

    data_manager = DataManager()
    data_manager.initialize_data(problem, [dataset], view='TEST')

    hp = ExecutionHelper(problem, data_manager)

    initial_testdata = data_manager.input_data

#    print(config["temp_storage_root"])

#    print(data_manager.input_data)

#    print(data_manager.target_data)

     

    for ppln in controller.exec_pipelines:
        primfile_base = config["temp_storage_root"] + "/models/" + ppln.id
        num_primitives = len(ppln.primitives)

#        print(primfile_base)

        curr_testdata = initial_testdata.copy()
        last_primitive = None

        for i in range(num_primitives):
            primfile = primfile_base + ".primitive_" + str(i + 1) + ".pkl"

#            print(primfile)

            primitive = sklearn.externals.joblib.load(primfile)
            last_primitive = primitive

#            print(primitive.task)

#            if i == num_primitives - 1:
#                continue

            if primitive.task == "PreProcessing":
                new_testdata = hp.test_execute_primitive(primitive, curr_testdata)
            elif primitive.task == "FeatureExtraction":
                new_testdata = hp.test_featurise(primitive, curr_testdata)
            elif primitive.task == "Modeling":
                continue

            curr_testdata = new_testdata.copy()

        assert last_primitive != None

        result = pandas.DataFrame(last_primitive.executables.produce(inputs=curr_testdata).value, index = curr_testdata.index, \
               columns = [cn["colName"] for cn in data_manager.target_columns]) 

#        See the range of results
        for col in [cn["colName"] for cn in data_manager.target_columns]:
            print("col = ", col, "min_val = ", result[col].min(), "max_val = ", result[col].max())

#        print(curr_testdata.shape)

#        print(last_primitive.executables.produce(inputs=curr_testdata).value[:10])
#        print(curr_testdata.index[:10])

#        for metric in controller.problem.metric_functions:
#            print(problem._get_metric_function(metric))
#        print(problem.metrics)

        # we want to compare result and data_manager.target_data
#        print("RESULT")
#        print(result)
#        print("DM - TARGET")
#        print(data_manager.target_data)

        # for each metric, compute the result
        for metric in problem.metrics:
            metric_function = problem._get_metric_function(metric)

            print(hp._call_function(metric_function, data_manager.target_data, result))
            print(hp._call_function(metric_function, result, data_manager.target_data))
#            print(metric_function([0,0], [0,1]))


        
                                    
if __name__ == "__main__":
    if DEBUG:
        sys.argv.append("-h")
        sys.argv.append("-v")
    sys.exit(main())
