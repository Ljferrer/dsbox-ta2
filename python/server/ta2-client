#!/usr/bin/env python

import sys
import os.path

# Setup Paths
PARENTDIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(PARENTDIR)

from dsbox_dev_setup import path_setup
path_setup()

import grpc
import urllib
import pandas

import pipeline_service_pb2 as ps
import pipeline_service_pb2_grpc as psrpc

def run():
    channel = grpc.insecure_channel('localhost:50051')
    stub = psrpc.PipelineComputeStub(channel)

    # Start Session
    session_response = stub.StartSession(
        ps.SessionRequest(user_agent="xxx", version="1.0"))
    session_context = session_response.context
    print("Session started (%s)" % str(session_context.session_id))

    # Send pipeline creation request
    train_features = [
        ps.Feature(
            data_uri="file:///Users/Varun/git/dsbox/data/o_185/data", feature_id="*")
    ]
    train_features_some = [
        ps.Feature(data_uri="file:///Users/Varun/git/dsbox/data/o_185/data",
                   feature_id="Games_played"),
        ps.Feature(
            data_uri="file:///Users/Varun/git/dsbox/data/o_185/data", feature_id="Runs"),
        ps.Feature(
            data_uri="file:///Users/Varun/git/dsbox/data/o_185/data", feature_id="Hits"),
        ps.Feature(data_uri="file:///Users/Varun/git/dsbox/data/o_185/data",
                   feature_id="Home_runs")
    ]
    task = ps.TaskType.Value('CLASSIFICATION')
    task_subtype = ps.TaskSubtype.Value('MULTICLASS')
    task_description = "Classify Hall of Fame"
    output = ps.OutputType.Value('FILE')
    metrics = [ps.Metric.Value('F1_MICRO')]
    target_features = [
        ps.Feature(
            data_uri="file:///Users/Varun/git/dsbox/data/o_185/data", feature_id="*")
    ]
    max_pipelines = 20

    print("Training with all features")
    pc_request = ps.PipelineCreateRequest(
        context=session_context,
        train_features=train_features,
        task=task,
        task_subtype=task_subtype,
        task_description=task_description,
        output=output,
        metrics=metrics,
        target_features=target_features,
        max_pipelines=max_pipelines
    )

    # Iterate over results
    pipeline_ids = []
    for pcr in stub.CreatePipelines(pc_request):
        print(str(pcr))
        if len(pcr.pipeline_info.scores) > 0:
            pipeline_ids.append(pcr.pipeline_id)

    '''
    print("Training with some features")
    pc_request = ps.PipelineCreateRequest(
        context = session_context,
        train_features = train_features_some,
        task = task,
        task_subtype = task_subtype,
        task_description = task_description,
        output = output,
        metrics = metrics,
        target_features = target_features,
        max_pipelines = max_pipelines
    )

    # Iterate over results
    pipeline_id = None
    for pcr in stub.CreatePipelines(pc_request):
        print(str(pcr))
        pipeline_id = pcr.pipeline_id
    '''

    # Execute pipelines
    for pipeline_id in pipeline_ids:
        print("Executing Pipeline %s" % pipeline_id)
        ep_request = ps.PipelineExecuteRequest(
            context=session_context,
            pipeline_id=pipeline_id,
            predict_dataset_uris=["file:///Users/Varun/git/dsbox/data/o_185/data"]
        )
        for ecr in stub.ExecutePipeline(ep_request):
            print(str(ecr))
            df = pandas.read_csv(ecr.result_uris[0], index_col="d3mIndex")
            print(df)

    stub.EndSession(session_context)

if __name__ == '__main__':
    run()
